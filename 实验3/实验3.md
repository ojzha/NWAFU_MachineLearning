 <center><font size='5'><b>实验3　SVM分类器</b></font></center>

## 1. 实验目的

1、掌握线性支持向量机（SVM）分类器。

2、掌握基于高斯核的 SVM 分类器。

3、掌握基于拉普拉斯核的 SVM 分类器。

## 2. 数据集介绍

| 数据集 | 样本数 | 维度 | 类数 |  数据类型  |
| :----: | :----: | :--: | :--: | :--------: |
| MINIST |  3000  | 784  |  10  | 手写体数字 |
|  Yale  |  165   | 1024 |  15  |  人脸图像  |
|  lung  |  203   | 3312 |  5   |  生物数据  |

数据集中的labels都已经展平为一维数据。

## 3. 实验内容

1、编写程序实现线性 SVM 分类器设计。

2、编写程序实现基于高斯核的 SVM 分类器设计。

3、编写程序实现基于拉普拉斯核的 SVM 分类器设计。

## 4. 实验结果

### 4.1 SVM模型原理

##### 超平面与间隔

- **超平面**：在$n$维空间中，超平面方程为：
  $$ \mathbf{w}^T \mathbf{x} + b = 0 $$
  其中$\mathbf{w}$是法向量，$b$是偏置项。

- **间隔（Margin）**：数据点到超平面的距离：
  $$ \text{Distance} = \frac{|\mathbf{w}^T \mathbf{x} + b|}{\|\mathbf{w}\|} $$
  SVM目标是最大化最小间隔。
  
  <img src="C:\Users\13158\AppData\Roaming\Typora\typora-user-images\image-20250517160704239.png" alt="image-20250517160704239" style="zoom:25%;" />

##### 硬间隔SVM
优化问题：
$$
\begin{cases}
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{s.t. } y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad \forall i
\end{cases}
$$

##### 软间隔SVM
引入松弛变量$\xi_i$和惩罚参数$C$：
$$
\begin{cases}
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i} \xi_i \\
\text{s.t. } y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{cases}
$$

### 4.2 本实验所用核函数介绍

在SVM中，**核函数**的作用是将原始数据**从低维空间映射到高维特征空间**，从而使非线性可分的问题在高维空间中变得线性可分。通过核函数，SVM无需显式地进行高维映射，只需计算样本之间在高维空间中的内积，即可构建非线性的分类或回归模型。

#### 4.2.1 线性核
##### 公式
$ K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j $

##### 特点
- **无参数**：无需调整超参数，计算简单高效。
- **线性决策边界**：直接在原始特征空间中构造线性超平面，适用于线性可分数据。

##### 应用场景
- 特征数量多（如文本分类）时表现良好。
- 数据本身线性可分或近似线性可分时优先使用。

##### 优点
- 计算速度快，不易过拟合。

##### 缺点
- 无法处理复杂非线性关系。

---

#### 4.2.2 高斯核（RBF核）
##### 公式
$ K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2\right) $

- $\gamma = \frac{1}{2\sigma^2}$ 控制核的宽度（$\gamma$ 越大，模型越复杂）。

##### 特点
- **非线性映射**：将数据映射到无限维空间，可拟合复杂边界。
- **局部性敏感**：样本距离越近，核函数值越大（最大值为1，距离远时趋近0）。

##### 应用场景
- 适用于无明显先验知识或特征间存在复杂非线性关系的数据。

##### 优点

- 灵活性强，可处理高度非线性数据。

##### 缺点
- 需调参（$\gamma$ 和 $C$），计算成本较高。
- $\gamma$ 过大会导致过拟合，过小会欠拟合。

---

#### 4.2.3 拉普拉斯核
##### 公式
$ K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|_1\right) $

- 使用 $L_1$ 范数（曼哈顿距离），$\gamma$ 控制核的缩放。

##### 特点
- **对异常值更鲁棒**：相比高斯核的 $L_2$ 范数，$L_1$ 范数对离群点不敏感。
- **衰减更慢**：距离增大时，核值衰减速度比高斯核慢。

##### 应用场景
- 数据存在噪声或异常值时。
- 需要比高斯核更宽松的相似性度量时。

##### 优点
- 鲁棒性强，适合高维稀疏数据。

##### 缺点
- 计算效率略低于高斯核（因 $L_1$ 范数计算成本较高）。



### 4.3 建模结果

#### 4.2.1 MINIST 数据集

<img src="D:\Master\专业课程学习\机器学习\实验\实验3\images\拼图_01.png" alt="拼图_01" style="zoom:100%;" />

<center><font size='3'><b>图1　MINIST 数据集使用不同核函数时预测结果的混淆矩阵：（a）线性核，（b）rbf核，（c）拉普拉斯核</b></font></center>

#### 4.2.2 Yale 数据集

<img src="D:\Master\专业课程学习\机器学习\实验\实验3\images\拼图_02.png" alt="拼图_02" style="zoom:150%;" />

<center><font size='3'><b>图2　Yale 数据集使用不同核函数时预测结果的混淆矩阵：（a）线性核，（b）rbf核，（c）拉普拉斯核</b></font></center>

#### 4.2.3 lung 数据集

#### 

![拼图_03](D:\Master\专业课程学习\机器学习\实验\实验3\images\拼图_03.png)

<center><font size='3'><b>图2　lung 数据集使用不同核函数时预测结果的混淆矩阵：（a）线性核，（b）rbf核，（c）拉普拉斯核</b></font></center>

#### 

#### 4.2.4 三数据集Accuracy结果对比

各个核函数在不同数据集预测结果的Acc结果如表1所示，MNIST和Lung数据集在线性核下均取得最高准确率（0.90和0.95），说明手写数字的像素特征和医学影像的病理特征可能具有线性可分性，而拉普拉斯核在MNIST的等效表现（0.90）暗示其曼哈顿距离计算对稀疏特征具有兼容性。Yale数据集整体表现较弱（最高Acc仅0.64），且三类核函数结果趋同，反映人脸数据的光照、姿态等非线性干扰未被有效捕捉，可能需要更复杂的特征编码或数据增强。

<center><font size='3'><b>表1　各个核函数在不同数据集预测结果的Acc对比</b></font></center>

| 数据集名称 | SVM核函数 | Acc  |
| :--------: | :-------: | :--: |
|   MNIST    |   线性    | 0.90 |
|            |   高斯    | 0.88 |
|            | 拉普拉斯  | 0.90 |
|    Yale    |   线性    | 0.64 |
|            |   高斯    | 0.62 |
|            | 拉普拉斯  | 0.62 |
|    Lung    |   线性    | 0.95 |
|            |   高斯    | 0.93 |
|            | 拉普拉斯  | 0.87 |

图4揭示了模型行为之间的特差异性。Lung数据集在线性核下精确率接近1但召回率相对波动（0.85-0.92），说明模型对阳性样本判断严格但存在漏检风险；Yale数据集在非线性核下的召回率塌陷（最低0.53）与精确率虚高（0.95）形成“伪精准”陷阱，提示类别不平衡问题；而MNIST各项指标的稳定性（F1集中在0.89-0.91）验证了该数据集的低歧义特性，为核函数选择提供了更大容错空间。

<img src="D:\Master\专业课程学习\机器学习\实验\实验3\images\metric_trends.png" alt="metric_trends" style="zoom:24%;" />

<center><font size='3'><b>图4　各个核函数在不同数据集预测结果的P、R和F1分数对比</b></font></center>

## 5 实验感悟

通过本次基于SVM核方法的分类实验，我深刻体会到不同核函数在特征空间映射中的独特作用。在MNIST数据集上，线性核与拉普拉斯核均取得90%的准确率，这让我直观认识到核函数的选择本质上是对数据结构的假设：线性核假设数据线性可分，而拉普拉斯核通过L1距离度量更适合稀疏特征。实验中对比不同核函数的性能表现，让我清楚看到没有绝对优越的核函数，关键在于理解数据分布特性与核函数数学性质的匹配程度。